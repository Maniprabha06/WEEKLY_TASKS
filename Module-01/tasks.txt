1.Unix Internals :
    Unix is a multiuser,multitasking OS designed for flexibility and adaptability
    Originally developed in 1960s
    Written in C programming language
    
    Imagine Kernal as a city with two distinct zones.
    User space is a bustling downtown where applications operate (like web browser or text editor)
    Kernal space is like a city control center,which is managed by city council called Kernal where it manages the traffic (I/O) and Security.
    
    When a application needs to access a file. it should  "request" the kernal (the system call)!!
    The kernal then handles actual file access,ensuring security and resource management.

    HANDS-ON:
    - Write a simple C program that uses the write() system call to write a message to the standard output (stdout).
    step 1:
        for installing gcc to run c program...and strace for system call!
        # sudo apt update
        # sudo apt install gcc strace -y
    step 2:
        create a c file to write a program.
        I just created using nano command for terminal based text editor
        # nano demo_program
    step 3:
    program that uses write() system call:
        #include <uinstd.h>
        #include <string.h>
        int main(){
            const char *text="Pre-Onboard Learning Module-01\n";
            write(1,text,strlen(text));  //1 for stdout ,can use 2 for stderr
            return 0;
        }
    Saved the file.
    step 4:
    # gcc demp_program -o demo_program    //create a executable file
    # ./demo_program  //it shows std output 

    - Use the strace command to observe the system calls made by your program.
    # strace ./demo_program  //to trace the system call

    - Explain which parts of the program operate in user space and which parts involve the kernel space.
        #include <uinstd.h>   //user space
        #include <string.h>   //user space
        int main(){           //user space
            const char *text="Pre-Onboard Learning Module-01\n";  //user space
            write(1,text,strlen(text));  // strlen(text)->user space write -> kernal space...starts with user space call that invokes kernal space
            return 0; //user space
        }

    - Extension: Research and explain the difference between a system call and a library function.
    System calls:
        - It is a direct request made by a user program to a OS kernal
        - user space can't directly access the hardware.. so it uses kernal to make request via a system calls
        - It is used when user program needs acces for low level resources like files,memory,devies or processes.
        - it can be traced using strace
        - Direct OS interactions
        eg: write(),read(),open(),fork(),exit(),execve()

    library functions:
        - it's a part of language's standard library
        - convenient wrapper
        - it is a helper function which may or may not internally use system calls
        - faster when compared to system calls.
        - runs entirely in user space
        eg: printf(),malloc(),fopen(),strlen()


2. Processor_speed:
    - The speed at which a processor can execute instructions.
    - processor(CPU) ->chef , instruction per second ->chef's cooking speed , dishes -> operations , recipe -> instructions
    - simple task -> less time, complex task -> more executing time
    - efficient code -> CPU Will Benefits

    HANDS-ON:
    - Write a C program that performs a large number of simple arithmetic operations (e.g., addition, multiplication) in a loop.
        step 1:
            # nano processor_speed.C //a terminal based editor will open
        step 2:
        PROGRAM:
            #include <stdio.h>
            int main(){
                long long int i,result=0;
                for(i=0;i<1000000;i++){
                    result+=i*3;
                }
                printf("result : %lld\n",result);
                return 0;
            }
        step 3:
        # gcc processor_speed.c -o processor_speed //executable file

    - Use the time command to measure the execution time of your program.
        # time ./processor_speed //shows output

    - Modify the program to perform more complex operations or increase the loop iterations.
        # nano processor_speed.C

    Modified PROGRAM:

        for(i=0;i<200000000;i++){
            result+=(i*2)/3 + (i%7);
        }

            # gcc processor_speed.c -o processor_speed
            # time ./processor_speed // shows output with more executing time

    - Measure the new execution time and discuss how the processor's speed affects the overall performance.
        - if I increase the complexity of the operation it results in longer execution time.
        - Optimizing the code can increase the performance of the CPU.
        - The Processor gets benefited from efficient code.


    - Extension: Research and explain CPU clock cycles, and how that relates to instructions per second.
        CLOCK CYCLES:
        - It is the smallest unit of time in the CPU.
        - It is like a heartbeat that tells the CPU when to do something.
        - CPU performs action sync with the Clock cycle.
        - It is measured in Hertz(Hz) or cycles per second.

        INSTRUCTIONS PER SECOND:
        - Instructions per second (IPS) indicate how many instructions a CPU can execute in one second.
        - A higher IPS means the CPU can process more commands in the same amount of time.

                                       Clock speed
        Instruction per second = -----------------------
                                 cycles per Instruction

        CPU runs at 2 GHz and each instruction it takes on average 2 cycles,then

                                    2,000,000,000
        Innstruction per second = ------------------ = 1,000,000,000
                                          2

        - It takes 1 billion Instruction per second.


3.Multi-Core Processors:
    - multiple processes run parallely
    - they can handle more tasks simultaneously. 
    - improving overall performance and efficiency.

    HANDS-ON:
    - Write a Python program that performs a time-consuming task (e.g., calculating prime numbers) in a single thread.
        # sudo apt update
        # sudo apt install python3-pip
        # nano prime_count.py
        PROGRAM:
        import time
        def is_prime(n):
            if n <= 1:
                return False
            for i in range(2, int(n**0.5) + 1):
                if n % i == 0:
                    return False
            return True

        def count_prime(limit):
            count=0
            for num in range(2, limit + 1):
                if is_prime(num):
                    count += 1
            return count

        start=time.time()
        prime_count=count_prime(100000)
        end=time.time()

        print(f"Total primes : {prime_count}")
        print(f"time taken by single thread: {end-start:.2f} seconds")

        # python3 prime_count.py

    - Modify the program to use the threading module to perform the task in multiple threads.
        # nano prime_multithread.py

        PROGRAM
        import time
        import threading

        def is_prime(n):
            if n<=1:
                return False
            for i in range(2,int(n**0.5)+1):
                if n%i==0:
                    return False
            return True

        def count_prime(start,end,result,index):
            count=0;
            for i in range(start,end):
                if is_prime(i):
                    count+=1
            result[index]=count

        start_time=time.time()
        limit=100000
        mid=limit//2
        result=[0,0]

        t1=threading.Thread(target=count_prime,args(2,mid,result,0))
        t2=threading.Thread(target=count_prime,args(mid,limit,result,1))

        t1.start()
        t2.start()
        t1.join()
        t2.join()

        total_prime=result[0]+result[1]
        end_time=time.time()

        print(f"Total_prime :{total_prime})
        print(f"Time taken by multi thread : {end_time-start_time:.2f} seconds")
        
        # python3 prime_multithread.py

    - Measure the execution time of both versions and compare the results.
        - almost for my code it shows the same execution time but the multithreaded version is generally faster.
    
    - Discuss how multi-core processors can improve the performance of parallel tasks.
        # nano prime_multicore.py

        PROGRAM:
        import time
        import multiprocessing
        def is_prime(n):
            if n <= 1:
                return False
            for i in range(2, int(n**0.5) + 1):
                if n % i == 0:
                    return False
            return True

        def count_prime(start,end):
            count=0
            for num in range(start,end):
                if is_prime(num):
                    count += 1
            return count

        if __name__ :__main__:
            start=time.time()
            with multiprocessing.Pool(processes=2) as pool:
            result = pool.starmap(count_prime, [(2, 50000), (50000, 100000)])
            end=time.time()

            total=sum(result)
            print(f"Total prime: {total}")
            print(f"Time taken by multi-core: {end - start:.2f} seconds")

    - Extension: Use the python multiprocessing module, and compare the results of threading vs multiprocessing.
        MULTICORE:
            - By using multi core the timing is minimized for the same functions
            - Instead of waiting for one function to finish, the work is divided among cores
            - The total time can drop significantly, especially for heavy calculations


        Single thread -> slow -> one task at a time
        Multi thread -> same or slightly better -> In my case it shown the same time -> it has GIL Blocks
        Multicore -> fastest -> true parallelism

4.GPU & HOW IT WORKS:
    - Specialized for parallel processing(graphics,math)
    - it has thousands of small cores
    - Great for parallel task whereas CPU is suitable for sequential tasks
    - Optimized for high throughput
    - used for graphics-intensive application & machine learning

    - Install a simple GPU programming framework (e.g., CUDA or OpenCL).
        - I'm using Ubuntu in virtual box.
        - So I have installed OpenCL framework to try GPU code.
        
        # sudo apt update && sudo apt upgrade -y
        # sudo apt install ocl-icd-opencl-dev clinfo pocl-opencl-icd -y
        # clinfo  // to check if openCl is working
        # sudo apt install python3-pip
        # pip3 install pyopencl numpy

    - Write a basic GPU program that performs a simple vector addition.
        # nano vector_add.py
        # python3 vector_add.py
        PROGRAM:
        import pyopencl as cl
        import numpy as np
        import time

        N = 1000000  

        A = np.random.rand(N).astype(np.float32)
        B = np.random.rand(N).astype(np.float32)

        start_cpu = time.time()
        C_cpu = A + B
        end_cpu = time.time()

        platform = cl.get_platforms()[0]
        device = platform.get_devices()[0]
        ctx = cl.Context([device])
        queue = cl.CommandQueue(ctx)

        kernel_code = """
        __kernel void vector_add(__global float* A, __global float* B, __global float* C) {
            int i = get_global_id(0);
            C[i] = A[i] + B[i];
        }
        """

        program = cl.Program(ctx, kernel_code).build()

        mf = cl.mem_flags
        A_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A)
        B_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B)
        C_buf = cl.Buffer(ctx, mf.WRITE_ONLY, A.nbytes)

        start_gpu = time.time()
        program.vector_add(queue, A.shape, None, A_buf, B_buf, C_buf)
        queue.finish()
        end_gpu = time.time()

        C_gpu = np.empty_like(A)
        cl.enqueue_copy(queue, C_gpu, C_buf)

        print(" Match?" , np.allclose(C_cpu, C_gpu, atol=1e-6))
        print(" CPU Time: {:.6f} seconds".format(end_cpu - start_cpu))
        print(" GPU Time: {:.6f} seconds".format(end_gpu - start_gpu))

    - Compare the execution time of the GPU version with a CPU version of the same program.
        - I implemented vector addition and matrix multiplication using both CPU (pure Python/NumPy) and GPU (via OpenCL)
        - When comparing execution time, I observed that the CPU version consistently outperformed the GPU version
        - The reason behind this is...
            - My GPU code runs inside a VirtualBox VM, which may not use the physical GPU directly, or may default to a CPU-based OpenCL device.
            - The data transfer overhead and kernel launch time on GPU make it inefficient for small to medium workloads

    - Explain the advantages of using a GPU for parallel computations.
        - While GPUs are faster for large-scale, highly parallel computations.
        - they require significant setup overhead. 
        - Without access to direct GPU hardware, the CPU version will usually be faster. 
        - This experiment still helped me understand GPU programming, system architecture, and when to use each.

    - Extension: Implement a simple matrix multiplication on the GPU, and compare it's speed to the CPU.
        # nano vector_multi.py
        # python3 vector_multi.py

        PROGRAM:
        import numpy as np
        import pyopencl as cl
        import time
        N = 512

        A = np.random.rand(N, N).astype(np.float32)
        B = np.random.rand(N, N).astype(np.float32)
        
        start_cpu = time.time()
        C_cpu = np.matmul(A, B)
        end_cpu = time.time()
        cpu_time = end_cpu - start_cpu
        print("CPU Time:", cpu_time, "seconds")


        ctx = cl.create_some_context()
        queue = cl.CommandQueue(ctx)

        C_gpu = np.zeros((N, N), dtype=np.float32)


        mf = cl.mem_flags
        A_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A)
        B_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B)
        C_buf = cl.Buffer(ctx, mf.WRITE_ONLY, C_gpu.nbytes)


        kernel_code = """
        __kernel void matmul(const int N,
                            __global float* A,
                            __global float* B,
                            __global float* C) {
            int row = get_global_id(0);
            int col = get_global_id(1);
            float result = 0.0f;

            for (int k = 0; k < N; ++k) {
                result += A[row * N + k] * B[k * N + col];
            }
            C[row * N + col] = result;
        }
        """


        program = cl.Program(ctx, kernel_code).build()


        start_gpu = time.time()
        program.matmul(queue, (N, N), None, np.int32(N), A_buf, B_buf, C_buf)
        cl.enqueue_copy(queue, C_gpu, C_buf)
        queue.finish()
        end_gpu = time.time()
        gpu_time = end_gpu - start_gpu

        print("GPU Time:", gpu_time, "seconds")


        if gpu_time < cpu_time:
            print("\n GPU is faster by", cpu_time - gpu_time, "seconds")
        else:
            print("\n CPU is faster by", gpu_time - cpu_time, "seconds (may be due to OpenCL using CPU device)")

        - even for a 1024 x 1024 matrix the CPU outperformed the GPU.
        - Because i didn't used the physical GPU directly ..I have been using it through virtual box ubuntu setup.
        - When the system has access to real GPU ...The GPU will outperform the CPU.

5.Processes:
    - A process is a running instance of a program
    - if i run any command the system creates a process.
    - instance -> running copy of something.
    - Each process:
        - has it own memory
        - runs independently
        - managed by Operating system
        - each process has a thread,code,data.
    - a process is like a individual play performed on a stage.(A computer)
    
    - Write a shell script that starts multiple instances of a long-running program (e.g., a loop that prints numbers).
        # nano multi_instance.sh
        PROGRAM : multi_instance.sh
            bash count_limited.sh & pid1=$!
            bash count_limited.sh & pid2=$!
            bash count_limited.sh & pid3=$!

            echo "Started processes with PIDs:"
            echo "Process 1: $pid1"
            echo "Process 2: $pid2"
            echo "Process 3: $pid3"

        # nano count_limited.sh
        PROGRAM : count_limited.sh
            for((i=1;i<=50;i++))
            do
                echo "PID $$ - Count $i"
                sleep 1
            done

        # chmod +x count_limited.sh
        # chmod +x multi_instance.sh
        # ./multi_instance.sh

    - Use the ps and top commands to observe the running processes and their resource usage.
        # ps aux | grep count_limited.sh
        # top

        # watch "ps -ef | grep count_limited.sh | grep -v grep"  //to watch particular script in running state

    - Explain how each instance of the program is a separate process with its own memory and resources.
        - I ran a shell script multi_instance.sh,it launched a multiple process of count_limited.sh script at the same time.
        - each process ran independently with separate process IDs.
        - even though all the instance ran in the same code they have been treated as a separate process by OS.
        - Verified this by using :
            # ps -ef |  grep count_limited.sh
            # top
        - each process has:
            its own memory
            CPU usage
            resource
            execution timeline

       - Here I observed that running the same program creates independent processes not a shared ones.


    - Extension: Use the fork() system call in a C program to create child processes, and observe their behaviour.
        # nano fork_demo.c 
        PROGRAM : fork_demo.c 
            # include <stdio.h>
            # include <unistd.h>
            int main(){
                pid_t pid=fork();
                if(pid==0){
                    for(int i=0;i<5;i++){
                        printf("Child (PID %d): %d\n",getpid(),i);
                        sleep(1);
                    }
                }
                else{
                    for(int i=0;i<5;i++){
                        printf("Parent (PID %d): %d\n",getpid(),i);
                        sleep(1);
                    }
                }
                return 0;
            }

        # gcc fork_demo.c -o fork_demo
        # ./fork_demo
        
        # ps aux | grep fork_demo.c 
        # top
        # watch "ps -ef | grep fork_demo | grep -v -grep"

        - I used fork() system call to create child process from the parent process
        - fork() is a system call that creates a new process by duplicating the current process.
        - when i call the fork() system call the OS does the below:
            - duplicates the running program
            - Now i have two separate process:
                - original process -> parent
                - newly created process -> child
            - the two process run independentlyand simultaneously:
                - in parent -> fork() return a child's PID : 1235
                - in child -> fork() return a value 0
            - if pid==0 -> enters child block else parent block
            - each has a own space in memory.
            - each gets own copy of the program and runs independently.

6.Memory Layout & Management:
    - mameory layout is similar to a library with different sections.
    - code stored in script section
    - variables stored in props section
    - dynamically allocated memory in flexible space.
    - Understanding this helps while debugging tasks like segmentation fault and memory leaks.
    - efficient memory management is crucial for performance.

    Library:
    - code/text section - shelves holding scripts
    - Data section - shelves for global and static variables.
    - Heap - a flexible open space where you can request memory at runtime(malloc())
    - stack - shelves holding temporary variables created by functions.


    DIAGRAM:

                stack           -> local variable, function calls
          -----------------
                Heap            -> dynamic memory,malloc
          -----------------
            Initialized 
                data
          -----------------
              READ_ONLY 
                data
          -----------------
                Text            -> compile code 
               Segment

    - Write a C program that dynamically allocates memory using malloc() and forgets to free it (a memory leak).
        # nano memory_leak.c 
        PROGRAM : 
        #include <stdio.h>
        #include <stdlib.h>
        #include <uinstd.h>
        int main() {
            int *ptr = (int*) malloc(5 * sizeof(int)); 
            if (ptr == NULL) {
                printf("Memory allocation failed!\n");
                return 1;
            }

            for (int i = 0; i < 5; i++) {
                ptr[i] = i * 10;
            }

            printf("Values stored: ");
            for (int i = 0; i < 5; i++) {
                printf("%d ", ptr[i]);
            }
            printf("\n");
            printf("Sleeping for 30 seconds... check with pmap now!\n");
            sleep(30); 
            return 0;
        }

    - Use the valgrind tool to detect and analyze the memory leak.
        # sudo apt update
        # sudo apt install valgrind
        # gcc -g memory_leak.c -o memory_leak
        # valgrind ./memory_leak // it shows like definetely lost...memory leak...etc!!

    - Modify the program to correctly free the allocated memory.
        # nano leak_fixed.c 
        PROGRAM :
        #include <stdio.h>
        #include <stdlib.h>
        int main() {
            int *ptr = (int*) malloc(5 * sizeof(int)); 
            if (ptr == NULL) {
                printf("Memory allocation failed!\n");
                return 1;
            }

            for (int i = 0; i < 5; i++) {
                ptr[i] = i * 10;
            }

            printf("Values stored: ");
            for (int i = 0; i < 5; i++) {
                printf("%d ", ptr[i]);
            }
            printf("\n");
            free(ptr);
            return 0;
        }
        
        # gcc -g leak_fixed.c -o leak_fixed
        # valgrind ./leak_fixed  // no memory leaks.

    - Use pmap to view the memory map of a running process, and explain the different memory segments.
        # ps aux | grep memory_leak 
        // i will get a PID use this for pmap command
        # pmap <pid>
        It has:
            - code segment - text
            - Heap segment - malloc ,dynamic memory
            - stack segment - function call memory
            - libraries used - dependencies,std lib
        
    - Extension: Create a program that causes a segmentation fault by accessing memory out of bounds. Use gdb to debug the issue.

        # nano segfault.c 
        PROGRAM:
        #include <stdio.h>
        #include <stdlib.h>
        int main(){
            int *arr=NULL;
            *arr=42;
            return 0;
        }
        # gcc -g segfault.c -o segfault
        # sudo apt update
        # sudo apt install gdb
        # gdb ./segfault
        # run
        # backtrace
