1.Unix Internals :
    Unix is a multiuser,multitasking OS designed for flexibility and adaptability
    Originally developed in 1960s
    Written in C programming language
    
    Imagine Kernal as a city with two distinct zones.
    User space is a bustling downtown where applications operate (like web browser or text editor)
    Kernal space is like a city control center,which is managed by city council called Kernal where it manages the traffic (I/O) and Security.
    
    When a application needs to access a file. it should  "request" the kernal (the system call)!!
    The kernal then handles actual file access,ensuring security and resource management.

    HANDS-ON:
    - Write a simple C program that uses the write() system call to write a message to the standard output (stdout).
    step 1:
        for installing gcc to run c program...and strace for system call!
        # sudo apt update
        # sudo apt install gcc strace -y
    step 2:
        create a c file to write a program.
        I just created using nano command for terminal based text editor
        # nano demo_program
    step 3:
    program that uses write() system call:
        #include <uinstd.h>
        #include <string.h>
        int main(){
            const char *text="Pre-Onboard Learning Module-01\n";
            write(1,text,strlen(text));  //1 for stdout ,can use 2 for stderr
            return 0;
        }
    Saved the file.
    step 4:
    # gcc demp_program -o demo_program    //create a executable file
    # ./demo_program  //it shows std output 

    - Use the strace command to observe the system calls made by your program.
    # strace ./demo_program  //to trace the system call

    - Explain which parts of the program operate in user space and which parts involve the kernel space.
        #include <uinstd.h>   //user space
        #include <string.h>   //user space
        int main(){           //user space
            const char *text="Pre-Onboard Learning Module-01\n";  //user space
            write(1,text,strlen(text));  // strlen(text)->user space write -> kernal space...starts with user space call that invokes kernal space
            return 0; //user space
        }

    - Extension: Research and explain the difference between a system call and a library function.
    System calls:
        - It is a direct request made by a user program to a OS kernal
        - user space can't directly access the hardware.. so it uses kernal to make request via a system calls
        - It is used when user program needs acces for low level resources like files,memory,devies or processes.
        - it can be traced using strace
        - Direct OS interactions
        eg: write(),read(),open(),fork(),exit(),execve()

    library functions:
        - it's a part of language's standard library
        - convenient wrapper
        - it is a helper function which may or may not internally use system calls
        - faster when compared to system calls.
        - runs entirely in user space
        eg: printf(),malloc(),fopen(),strlen()


2. Processor_speed:
    - The speed at which a processor can execute instructions.
    - processor(CPU) ->chef , instruction per second ->chef's cooking speed , dishes -> operations , recipe -> instructions
    - simple task -> less time, complex task -> more executing time
    - efficient code -> CPU Will Benefits

    HANDS-ON:
    - Write a C program that performs a large number of simple arithmetic operations (e.g., addition, multiplication) in a loop.
        step 1:
            # nano processor_speed.C //a terminal based editor will open
        step 2:
        PROGRAM:
            #include <stdio.h>
            int main(){
                long long int i,result=0;
                for(i=0;i<1000000;i++){
                    result+=i*3;
                }
                printf("result : %lld\n",result);
                return 0;
            }
        step 3:
        # gcc processor_speed.c -o processor_speed //executable file

    - Use the time command to measure the execution time of your program.
        # time ./processor_speed //shows output

    - Modify the program to perform more complex operations or increase the loop iterations.
        # nano processor_speed.C

    Modified PROGRAM:

        for(i=0;i<200000000;i++){
            result+=(i*2)/3 + (i%7);
        }

            # gcc processor_speed.c -o processor_speed
            # time ./processor_speed // shows output with more executing time

    - Measure the new execution time and discuss how the processor's speed affects the overall performance.
        - if I increase the complexity of the operation it results in longer execution time.
        - Optimizing the code can increase the performance of the CPU.
        - The Processor gets benefited from efficient code.


    - Extension: Research and explain CPU clock cycles, and how that relates to instructions per second.
        CLOCK CYCLES:
        - It is the smallest unit of time in the CPU.
        - It is like a heartbeat that tells the CPU when to do something.
        - CPU performs action sync with the Clock cycle.
        - It is measured in Hertz(Hz) or cycles per second.

        INSTRUCTIONS PER SECOND:
        - Instructions per second (IPS) indicate how many instructions a CPU can execute in one second.
        - A higher IPS means the CPU can process more commands in the same amount of time.

                                       Clock speed
        Instruction per second = -----------------------
                                 cycles per Instruction

        CPU runs at 2 GHz and each instruction it takes on average 2 cycles,then

                                    2,000,000,000
        Innstruction per second = ------------------ = 1,000,000,000
                                          2

        - It takes 1 billion Instruction per second.


3.Multi-Core Processors:
    - multiple processes run parallely
    - they can handle more tasks simultaneously. 
    - improving overall performance and efficiency.

    HANDS-ON:
    - Write a Python program that performs a time-consuming task (e.g., calculating prime numbers) in a single thread.
        # sudo apt update
        # sudo apt install python3-pip
        # nano prime_count.py
        PROGRAM:
        import time
        def is_prime(n):
            if n <= 1:
                return False
            for i in range(2, int(n**0.5) + 1):
                if n % i == 0:
                    return False
            return True

        def count_prime(limit):
            count=0
            for num in range(2, limit + 1):
                if is_prime(num):
                    count += 1
            return count

        start=time.time()
        prime_count=count_prime(100000)
        end=time.time()

        print(f"Total primes : {prime_count}")
        print(f"time taken by single thread: {end-start:.2f} seconds")

        # python3 prime_count.py

    - Modify the program to use the threading module to perform the task in multiple threads.
        # nano prime_multithread.py

        PROGRAM
        import time
        import threading

        def is_prime(n):
            if n<=1:
                return False
            for i in range(2,int(n**0.5)+1):
                if n%i==0:
                    return False
            return True

        def count_prime(start,end,result,index):
            count=0;
            for i in range(start,end):
                if is_prime(i):
                    count+=1
            result[index]=count

        start_time=time.time()
        limit=100000
        mid=limit//2
        result=[0,0]

        t1=threading.Thread(target=count_prime,args(2,mid,result,0))
        t2=threading.Thread(target=count_prime,args(mid,limit,result,1))

        t1.start()
        t2.start()
        t1.join()
        t2.join()

        total_prime=result[0]+result[1]
        end_time=time.time()

        print(f"Total_prime :{total_prime})
        print(f"Time taken by multi thread : {end_time-start_time:.2f} seconds")
        
        # python3 prime_multithread.py

    - Measure the execution time of both versions and compare the results.
        - almost for my code it shows the same execution time but the multithreaded version is generally faster.
    
    - Discuss how multi-core processors can improve the performance of parallel tasks.
        # nano prime_multicore.py

        PROGRAM:
        import time
        import multiprocessing
        def is_prime(n):
            if n <= 1:
                return False
            for i in range(2, int(n**0.5) + 1):
                if n % i == 0:
                    return False
            return True

        def count_prime(start,end):
            count=0
            for num in range(start,end):
                if is_prime(num):
                    count += 1
            return count

        if __name__ :__main__:
            start=time.time()
            with multiprocessing.Pool(processes=2) as pool:
            result = pool.starmap(count_prime, [(2, 50000), (50000, 100000)])
            end=time.time()

            total=sum(result)
            print(f"Total prime: {total}")
            print(f"Time taken by multi-core: {end - start:.2f} seconds")

    - Extension: Use the python multiprocessing module, and compare the results of threading vs multiprocessing.
        MULTICORE:
            - By using multi core the timing is minimized for the same functions
            - Instead of waiting for one function to finish, the work is divided among cores
            - The total time can drop significantly, especially for heavy calculations


        Single thread -> slow -> one task at a time
        Multi thread -> same or slightly better -> In my case it shown the same time -> it has GIL Blocks
        Multicore -> fastest -> true parallelism

4.GPU & HOW IT WORKS:
    - Specialized for parallel processing(graphics,math)
    - it has thousands of small cores
    - Great for parallel task whereas CPU is suitable for sequential tasks
    - Optimized for high throughput

    - Install a simple GPU programming framework (e.g., CUDA or OpenCL).
        - I'm using Ubuntu in virtual box.
        - So I have installed OpenCL framework to try GPU code.
        
        # sudo apt update && sudo apt upgrade -y
        # sudo apt install ocl-icd-opencl-dev clinfo pocl-opencl-icd -y
        # clinfo  // to check if openCl is working
        # sudo apt install python3-pip
        # pip3 install pyopencl numpy

    - Write a basic GPU program that performs a simple vector addition.
        # nano vector_add.py
        # python3 vector_add.py
        PROGRAM:
        import pyopencl as cl
        import numpy as np
        import time

        N = 1000000  

        A = np.random.rand(N).astype(np.float32)
        B = np.random.rand(N).astype(np.float32)

        start_cpu = time.time()
        C_cpu = A + B
        end_cpu = time.time()

        platform = cl.get_platforms()[0]
        device = platform.get_devices()[0]
        ctx = cl.Context([device])
        queue = cl.CommandQueue(ctx)

        kernel_code = """
        __kernel void vector_add(__global float* A, __global float* B, __global float* C) {
            int i = get_global_id(0);
            C[i] = A[i] + B[i];
        }
        """

        program = cl.Program(ctx, kernel_code).build()

        mf = cl.mem_flags
        A_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A)
        B_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B)
        C_buf = cl.Buffer(ctx, mf.WRITE_ONLY, A.nbytes)

        start_gpu = time.time()
        program.vector_add(queue, A.shape, None, A_buf, B_buf, C_buf)
        queue.finish()
        end_gpu = time.time()

        C_gpu = np.empty_like(A)
        cl.enqueue_copy(queue, C_gpu, C_buf)

        print(" Match?" , np.allclose(C_cpu, C_gpu, atol=1e-6))
        print(" CPU Time: {:.6f} seconds".format(end_cpu - start_cpu))
        print(" GPU Time: {:.6f} seconds".format(end_gpu - start_gpu))

    - Compare the execution time of the GPU version with a CPU version of the same program.
    - Explain the advantages of using a GPU for parallel computations.
    - Extension: Implement a simple matrix multiplication on the GPU, and compare it's speed to the CPU.
        # nano vector_multi.py
        # python3 vector_multi.py

        PROGRAM:
        import numpy as np
        import pyopencl as cl
        import time
        N = 512

        A = np.random.rand(N, N).astype(np.float32)
        B = np.random.rand(N, N).astype(np.float32)
        
        start_cpu = time.time()
        C_cpu = np.matmul(A, B)
        end_cpu = time.time()
        cpu_time = end_cpu - start_cpu
        print("CPU Time:", cpu_time, "seconds")


        ctx = cl.create_some_context()
        queue = cl.CommandQueue(ctx)

        C_gpu = np.zeros((N, N), dtype=np.float32)


        mf = cl.mem_flags
        A_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A)
        B_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B)
        C_buf = cl.Buffer(ctx, mf.WRITE_ONLY, C_gpu.nbytes)


        kernel_code = """
        __kernel void matmul(const int N,
                            __global float* A,
                            __global float* B,
                            __global float* C) {
            int row = get_global_id(0);
            int col = get_global_id(1);
            float result = 0.0f;

            for (int k = 0; k < N; ++k) {
                result += A[row * N + k] * B[k * N + col];
            }
            C[row * N + col] = result;
        }
        """


        program = cl.Program(ctx, kernel_code).build()


        start_gpu = time.time()
        program.matmul(queue, (N, N), None, np.int32(N), A_buf, B_buf, C_buf)
        cl.enqueue_copy(queue, C_gpu, C_buf)
        queue.finish()
        end_gpu = time.time()
        gpu_time = end_gpu - start_gpu

        print("GPU Time:", gpu_time, "seconds")


        if gpu_time < cpu_time:
            print("\n GPU is faster by", cpu_time - gpu_time, "seconds")
        else:
            print("\n CPU is faster by", gpu_time - cpu_time, "seconds (may be due to OpenCL using CPU device)")





